{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eed9e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "sys.path.append(\"/Users/yuchenlin/Project/Tools/Asset-Pricing-Tools\")\n",
    "import basictool\n",
    "import anomalytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da80064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BacktestEngine...\n",
      "Loading raw data files...\n",
      "Loading factor models...\n",
      "Processing raw CRSP data...\n",
      "CRSP data processing complete.\n",
      "BacktestEngine initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "engine = anomalytest.BacktestEngine(stdate=datetime(1960,1,1),eddate=datetime(2024,12,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913d3047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing CRSP Data (Corrected Order) ---\n",
      "1. 原始行数: 2950365\n",
      "2. 筛选 Share Code (10,11) 后: 2532899\n",
      "3. 筛选 Exchange Code (1,2,3) 后: 2532899\n",
      "4. 计算并剔除 ME 缺失后: 2516613\n",
      "5. PERMCO 去重后: 2477099 (减少了 39514 行)\n",
      "--- 最终 CRSP 样本量: 2477099 ---\n"
     ]
    }
   ],
   "source": [
    "# def _process_crsp_data(self):\n",
    "#         print(\"--- Processing CRSP Data (Corrected Order) ---\")\n",
    "#         df = self.raw_crsp_m.copy()\n",
    "#         df = df[df['date']>=datetime(1963,1,1)]\n",
    "#         df = df[df['date']<=datetime(2005,12,31)]\n",
    "#         print(f\"1. 原始行数: {len(df)}\")\n",
    "\n",
    "#         # 格式化\n",
    "#         df['date'] = pd.to_datetime(df['date'])\n",
    "#         for col in ['permno', 'permco', 'shrcd', 'exchcd', 'siccd']: \n",
    "#             df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "#         df['prc'] = pd.to_numeric(df['prc'], errors='coerce').abs()\n",
    "#         df['ret'] = pd.to_numeric(df['ret'], errors='coerce')\n",
    "#         df['shrout'] = pd.to_numeric(df['shrout'], errors='coerce')\n",
    "\n",
    "#         # [关键调整] 1. 先进行资格筛选 (Filter First!)\n",
    "#         # 这一步会剔除大量的优先股、权证、基金，它们是造成\"假重复\"的元凶\n",
    "#         df = df[df['shrcd'].isin([10, 11])]\n",
    "#         print(f\"2. 筛选 Share Code (10,11) 后: {len(df)}\")\n",
    "        \n",
    "#         df = df[df['exchcd'].isin([1, 2, 3])]\n",
    "#         print(f\"3. 筛选 Exchange Code (1,2,3) 后: {len(df)}\")\n",
    "\n",
    "#         # [关键调整] 2. 基础清洗\n",
    "#         df.dropna(subset=['permno', 'date'], inplace=True) # 注意：me 需要先计算\n",
    "#         # 这里需要先算一下 ME，因为你的代码逻辑依赖 ME\n",
    "#         df['me'] = df['prc'] * df['shrout'] / 1000\n",
    "#         df.dropna(subset=['me'], inplace=True)\n",
    "#         print(f\"4. 计算并剔除 ME 缺失后: {len(df)}\")\n",
    "\n",
    "#         # [关键调整] 3. 处理 Permco 并去重 (Deduplicate Last!)\n",
    "#         # 修复 Permco 缺失\n",
    "#         df['permco'] = df['permco'].fillna(df['permno'])\n",
    "        \n",
    "#         # 聚合市值 (处理真正的多重股权，如 Google A/C)\n",
    "#         df_me_sum = df.groupby(['date', 'permco'])['me'].sum().reset_index(name='me_permco_total')\n",
    "#         df = pd.merge(df, df_me_sum, on=['date', 'permco'], how='left')\n",
    "        \n",
    "#         # 排序：保留市值最大的那个 Permno 代表该公司\n",
    "#         df.sort_values(by=['date', 'permco', 'me'], ascending=[True, True, False], inplace=True)\n",
    "        \n",
    "#         before_dedup = len(df)\n",
    "#         df = df.drop_duplicates(subset=['date', 'permco'], keep='first')\n",
    "#         print(f\"5. PERMCO 去重后: {len(df)} (减少了 {before_dedup - len(df)} 行)\")\n",
    "#         # 此时减少的数量应该非常少 (<5%)\n",
    "        \n",
    "#         df['me'] = df['me_permco_total']\n",
    "#         df.drop(columns=['me_permco_total'], inplace=True)\n",
    "\n",
    "#         # ... 后续处理 (Ret >= -1 清洗等) ...\n",
    "#         df.loc[df['ret'] < -1, 'ret'] = np.nan\n",
    "        \n",
    "#         print(f\"--- 最终 CRSP 样本量: {len(df)} ---\")\n",
    "#         #return df.reset_index(drop=True), None # dec_me 在外部处理\n",
    "# _process_crsp_data(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a807645",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = '~/Raw_data/'\n",
    "\n",
    "FILE_PATHS = {\n",
    "    \"compustat_annual\": raw_path + \"COMPUSTAT/funda.csv\",  \n",
    "    \"crsp_monthly\": raw_path + \"CRSP/msf.csv\",      \n",
    "    \"ccm_link_table\": raw_path + \"CRSP/ccmxpf_lnkhist.csv\",\n",
    "    \"compustat_quarter\" :  raw_path + \"COMPUSTAT/fundq.csv\"\n",
    "}\n",
    "FRED_PATH = raw_path + 'FRED/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62ccad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wrds_dates(series):\n",
    "    \"\"\"\n",
    "    统一清洗 WRDS 导出的日期列。\n",
    "    处理 'E', '99999999' -> 未来日期\n",
    "    处理 'B', '00000000' -> 过去日期\n",
    "    处理其他无法解析的字符 -> NaT\n",
    "    \"\"\"\n",
    "    # 1. 替换特殊代码 (根据你的数据情况，可以添加更多映射)\n",
    "    replacements = {\n",
    "        'E': '2099-12-31',\n",
    "        '99999999': '2099-12-31',\n",
    "        'C': '2099-12-31', # 有时 C 也代表 Current\n",
    "        'B': '1900-01-01',\n",
    "        '00000000': '1900-01-01'\n",
    "    }\n",
    "    \n",
    "    # 2. 执行替换\n",
    "    # 注意：先转成字符串处理，防止原本已经是混合类型\n",
    "    series = series.astype(str).replace(replacements)\n",
    "    \n",
    "    # 3. 转换为日期格式\n",
    "    # errors='coerce' 会把剩下的无法识别的乱码变成 NaT\n",
    "    return pd.to_datetime(series, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fe501d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statutory_tax_rate(year):\n",
    "    \"\"\"\n",
    "    返回美国联邦法定最高企业所得税率 (1963-2020)\n",
    "    数据来源: IRS / Tax Foundation\n",
    "    \"\"\"\n",
    "    if year <= 1963:\n",
    "        return 0.52\n",
    "    elif year == 1964:\n",
    "        return 0.50\n",
    "    elif 1965 <= year <= 1967:\n",
    "        return 0.48\n",
    "    elif 1968 <= year <= 1969:\n",
    "        return 0.528  # 包含附加税\n",
    "    elif year == 1970:\n",
    "        return 0.492\n",
    "    elif 1971 <= year <= 1978:\n",
    "        return 0.48\n",
    "    elif 1979 <= year <= 1986:\n",
    "        return 0.46\n",
    "    elif year == 1987:\n",
    "        return 0.40   # 1986税改过渡\n",
    "    elif 1988 <= year <= 1992:\n",
    "        return 0.34\n",
    "    elif 1993 <= year <= 2017:\n",
    "        return 0.35\n",
    "    elif year >= 2018:\n",
    "        return 0.21\n",
    "    else:\n",
    "        return 0.35 # 默认值，防止报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e76080a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_future_yearly(df: pd.DataFrame, var_list: list, diff_list: list, lag: int = 1) -> pd.DataFrame:\n",
    "\n",
    "    if not all(col in df.columns for col in ['gvkey', 'fyear']):\n",
    "        raise ValueError(\"输入DataFrame必须包含 'gvkey' 和 'fyear' 列。\")\n",
    "\n",
    "    # 创建左右两个表，与SAS的 `a` 和 `b` 完全对应\n",
    "    df_left = df.copy()\n",
    "    df_right = df[['gvkey', 'fyear'] + var_list].copy()\n",
    "\n",
    "    # 准备右表的连接键，与 a.FYEAR = b.FYEAR + lag 对应\n",
    "    df_right['fyear_join_key'] = df_right['fyear'] - lag\n",
    "    \n",
    "    # 重命名右表的列，以 '_lag' 结尾\n",
    "    lag_col_names = {var: f\"{var}_t{lag}\" for var in var_list}\n",
    "    df_right.rename(columns=lag_col_names, inplace=True)\n",
    "    \n",
    "    # 执行 LEFT JOIN，与SAS的逻辑完全一致\n",
    "    merged_df = pd.merge(\n",
    "        df_left,\n",
    "        df_right.drop(columns='fyear'), # 丢弃原始fyear，只保留join_key\n",
    "        left_on=['gvkey', 'fyear'],\n",
    "        right_on=['gvkey', 'fyear_join_key'],\n",
    "        how='left' # <-- 明确使用 LEFT JOIN\n",
    "    )\n",
    "    \n",
    "    if not diff_list:\n",
    "        for var in diff_list:\n",
    "            merged_df[\"{}_diff{}\".format(var,lag)] = merged_df[\"{}\".format(var)] - merged_df['{}_lag{}'.format(var,lag)]\n",
    "\n",
    "    return merged_df.drop(columns='fyear_join_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "367b96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lag_yearly(df: pd.DataFrame, var_list: list, diff_list: list, lag: int = 1) -> pd.DataFrame:\n",
    "\n",
    "    if not all(col in df.columns for col in ['gvkey', 'fyear']):\n",
    "        raise ValueError(\"输入DataFrame必须包含 'gvkey' 和 'fyear' 列。\")\n",
    "\n",
    "    # 创建左右两个表，与SAS的 `a` 和 `b` 完全对应\n",
    "    df_left = df.copy()\n",
    "    df_right = df[['gvkey', 'fyear'] + var_list].copy()\n",
    "\n",
    "    # 准备右表的连接键，与 a.FYEAR = b.FYEAR + lag 对应\n",
    "    df_right['fyear_join_key'] = df_right['fyear'] + lag\n",
    "    \n",
    "    # 重命名右表的列，以 '_lag' 结尾\n",
    "    lag_col_names = {var: f\"{var}_lag{lag}\" for var in var_list}\n",
    "    df_right.rename(columns=lag_col_names, inplace=True)\n",
    "    \n",
    "    # 执行 LEFT JOIN，与SAS的逻辑完全一致\n",
    "    merged_df = pd.merge(\n",
    "        df_left,\n",
    "        df_right.drop(columns='fyear'), # 丢弃原始fyear，只保留join_key\n",
    "        left_on=['gvkey', 'fyear'],\n",
    "        right_on=['gvkey', 'fyear_join_key'],\n",
    "        how='left' # <-- 明确使用 LEFT JOIN\n",
    "    )\n",
    "    \n",
    "    if not diff_list:\n",
    "        for var in diff_list:\n",
    "            merged_df[\"{}_diff{}\".format(var,lag)] = merged_df[\"{}\".format(var)] - merged_df['{}_lag{}'.format(var,lag)]\n",
    "\n",
    "    return merged_df.drop(columns='fyear_join_key')\n",
    "\n",
    "def _load_required_data(paths: dict):\n",
    "    \"\"\"\n",
    "    内部辅助函数，用于从指定路径加载所有必需的数据文件。\n",
    "    \"\"\"\n",
    "    print(\"Loading required data files...\")\n",
    "    cols = ['gvkey','indfmt','datafmt','consol','fic','datadate','sic','fyear','fyr','at','capx','ppegt','dlc','dltt','dp','sppe','seq','txditc','pstkrv', 'pstkl', 'pstk','sale']\n",
    "    try:\n",
    "        compustat_annual = pd.read_csv(paths[\"compustat_annual\"], low_memory=False,usecols=cols,dtype={'gvkey':str})\n",
    "        numetic_cols = ['fyear','fyr','at','capx','ppegt','dlc','dltt','dp','sppe','seq','txditc','pstkrv', 'pstkl', 'pstk','sale']\n",
    "        for col in numetic_cols:\n",
    "            if col in compustat_annual.columns:\n",
    "            # errors='coerce' 是关键：它会把无法转成数字的字符（如 'A', 'C'）直接变成 NaN\n",
    "                compustat_annual[col] = pd.to_numeric(compustat_annual[col], errors='coerce')\n",
    "        date_cols = ['datadate', 'linkdt', 'linkenddt', 'ipodate', 'dldte']\n",
    "\n",
    "        for col in date_cols:\n",
    "            if col in compustat_annual.columns:\n",
    "                compustat_annual[col] = clean_wrds_dates(compustat_annual[col])\n",
    "\n",
    "        crsp_monthly = engine.crsp_m_processed.copy()\n",
    "\n",
    "        ccm_link_table = pd.read_csv(paths[\"ccm_link_table\"], low_memory=False,dtype={'gvkey':str})\n",
    "        ccm_link_table.columns = [i.lower() for i in ccm_link_table.columns]\n",
    "        for col in date_cols:\n",
    "            if col in ccm_link_table.columns:\n",
    "                ccm_link_table[col] = clean_wrds_dates(ccm_link_table[col])\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"错误：文件未找到。请确保在 FILE_PATHS 字典中提供了正确的路径。\")\n",
    "        raise e\n",
    "        \n",
    "    print(\"All data files loaded successfully.\")\n",
    "    return compustat_annual, crsp_monthly, ccm_link_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9623ba21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required data files...\n",
      "All data files loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "compa, crspm, ccm_lnk = _load_required_data(FILE_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "190711ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annual_anomalies(\n",
    "    compustat_annual: pd.DataFrame,\n",
    "    crsp_monthly: pd.DataFrame,\n",
    "    ccm_link_table: pd.DataFrame\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        compustat_annual (pd.DataFrame): Compustat年度财务数据 (funda)。\n",
    "            必需列: gvkey, datadate, fyear, \n",
    "                     \n",
    "        crsp_monthly (pd.DataFrame): 经过预处理的CRSP月度数据 (如BacktestEngine的产出)。\n",
    "            必需列: permno, date, me, exchcd.\n",
    "            \n",
    "        ccm_link_table (pd.DataFrame): CRSP-Compustat合并链接表。\n",
    "            必需列: gvkey, lpermno, linkdt, linkenddt, linktype, linkprim.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Annual Investment Anomaly Creation ---\")\n",
    "\n",
    "    # --- 步骤 1: 处理Compustat年度数据 (应用“黄金过滤器”并计算变量) ---\n",
    "    print(\"Step 1: Processing Compustat annual data...\")\n",
    "    comp = compustat_annual.copy()\n",
    "    comp['datadate'] = pd.to_datetime(comp['datadate'])\n",
    "\n",
    "    # 1a. 应用Compustat“黄金过滤器”\n",
    "    comp = comp[\n",
    "        (comp['indfmt'] == 'INDL') & (comp['datafmt'] == 'STD') &\n",
    "        (comp['consol'] == 'C') &\n",
    "        (comp['fic'] == 'USA') & comp['gvkey'].notna() & comp['fyear'].notna()\n",
    "    ].copy()\n",
    "\n",
    "    # --- 步骤 2: 处理变量是否用0填充以及计算diff---    \n",
    "    na_list = ['sppe','txditc','dlc','dltt']\n",
    "    for na_var in na_list:\n",
    "        comp[na_var] = comp[na_var].fillna(0)\n",
    "    \n",
    "    lag_list = ['ppegt','at']\n",
    "    diff_list = []\n",
    "\n",
    "    # Construct BE\n",
    "    comp['ps']=np.where(comp['pstkrv'].isnull(), comp['pstkl'], comp['pstkrv'])\n",
    "    comp['ps']=np.where(comp['ps'].isnull(),comp['pstk'], comp['ps'])\n",
    "    comp['ps']=np.where(comp['ps'].isnull(),0,comp['ps'])\n",
    "\n",
    "    comp['be'] = comp['seq'] + comp['txditc'] - comp['ps']\n",
    "    comp['be']=np.where(comp['be']>0, comp['be'], np.nan)\n",
    "    \n",
    "    comp = compute_lag_yearly(comp,lag_list,diff_list,1)\n",
    "\n",
    "    # Construct firm variables \n",
    "    comp['ce'] = comp['capx'] / comp['sale']\n",
    "    comp['debt'] = comp['dlc'] + comp['dltt']\n",
    "    comp['delta'] = comp['dp'] / comp['ppegt_lag1']\n",
    "    comp['capital'] = comp['ppegt_lag1']\n",
    "    comp['investment'] = comp['capx'] - comp['sppe']\n",
    "    comp['investment_alter'] = comp['at'] / comp['at_lag1'] - 1\n",
    "    comp['output'] = comp['sale']\n",
    "\n",
    "    comp = compute_lag_yearly(comp,['ce'],[],1)\n",
    "    comp = compute_lag_yearly(comp,['ce'],[],2)\n",
    "    comp = compute_lag_yearly(comp,['ce'],[],3)\n",
    "    \n",
    "    comp['ci'] = comp['ce'] / (comp['ce_lag1'] + comp['ce_lag2'] + comp['ce_lag3']) * 3\n",
    "    comp.loc[comp['sale']<=0, 'ce'] = np.nan # 避免除以0\n",
    "    # --- 步骤 3: 构建变量 ---\n",
    "\n",
    "    # --- 步骤 4: 准备并链接CRSP-Compustat (CCM) -z-\n",
    "    print(\"Step 2: Preparing and linking via CCM table...\")\n",
    "    ccm = ccm_link_table[\n",
    "        (ccm_link_table['linktype'].isin(['LU', 'LC'])) &\n",
    "        (ccm_link_table['linkprim'].isin(['P', 'C']))\n",
    "    ].copy()\n",
    "    ccm.rename(columns={'lpermno': 'permno'}, inplace=True)\n",
    "    ccm['linkdt'] = pd.to_datetime(ccm['linkdt'])\n",
    "    ccm['linkenddt'] = pd.to_datetime(ccm['linkenddt']).fillna(datetime(2099,12,30))\n",
    "\n",
    "\n",
    "    # 链接Compustat与CCM\n",
    "    comp_linked = pd.merge(comp, ccm, on='gvkey',how='inner')\n",
    "\n",
    "    # 筛选有效的链接期间\n",
    "    comp_linked = comp_linked[\n",
    "        (comp_linked['datadate'] >= comp_linked['linkdt']) &\n",
    "        (comp_linked['datadate'] <= comp_linked['linkenddt'])\n",
    "    ]\n",
    "\n",
    "\n",
    "    # --- 步骤 3: 创建CRSP的6月和12月锚点数据集 ---\n",
    "\n",
    "    print(\"Step 3: Creating CRSP June and December anchor subsets...\")\n",
    "    crsp = crsp_monthly.copy()\n",
    "    crsp['date'] = pd.to_datetime(crsp['date'])\n",
    "    \n",
    "    crsp_june = crsp[crsp['date'].dt.month == 6].copy()\n",
    "\n",
    "    dec_me = crsp[crsp['date'].dt.month == 12][['permno', 'date', 'me']].rename(columns={'me': 'dec_me'})\n",
    "\n",
    "    # --- 步骤 4: 执行“6月末”合并，并进行双重重复数据清洗 ---\n",
    "    print(\"Step 4: Performing end-of-June merge and de-duplication...\")\n",
    "    \n",
    "    # 4a. 创建合并键\n",
    "    crsp_june['year_merge'] = crsp_june['date'].dt.year\n",
    "    dec_me['year_merge'] = dec_me['date'].dt.year + 1\n",
    "    comp_linked['year_merge'] = comp_linked['fyear'] + 1\n",
    "\n",
    "    # 4b. 将6月CRSP数据与财务数据合并\n",
    "    merged = pd.merge(crsp_june, comp_linked, on=['permno', 'year_merge'])\n",
    "\n",
    "    # 4c. 第一层清洗：解决\"permno-date\" 匹配多个 \"gvkey\" 的问题\n",
    "    # 优先选择 linkprim = 'P' (Primary)\n",
    "    merged['linkprim'] = pd.Categorical(merged['linkprim'], categories=['C', 'P'], ordered=True)\n",
    "    merged.sort_values(by=['permno', 'date', 'linkprim'], ascending=[True, True, False], inplace=True)\n",
    "    merged = merged.drop_duplicates(subset=['permno', 'date'], keep='first')\n",
    "    \n",
    "    # 4d. 第二层清洗：解决公司变更财年导致同一年有多个财报的问题\n",
    "    merged['calendar_year'] = merged['datadate'].dt.year\n",
    "    merged.sort_values(by=['permno', 'calendar_year', 'datadate'], ascending=True, inplace=True)\n",
    "    merged = merged.drop_duplicates(subset=['permno', 'calendar_year'], keep='last')\n",
    "\n",
    "    # 4e. 合并12月市值 (DEC_ME)\n",
    "    final_merged = pd.merge(merged, dec_me[['permno', 'year_merge', 'dec_me']], on=['permno', 'year_merge'], how='left')\n",
    "\n",
    "    # --- 步骤 5: 计算最终的年度因子 ---\n",
    "    print(\"Step 5: Calculating final annual anomalies...\")\n",
    "    \n",
    "    # 账面市值比 (Book-to-Market)\n",
    "    final_merged['BM'] = final_merged['be'] / final_merged['dec_me']\n",
    "    \n",
    "    # 市值因子 (Market Equity, ME) - 使用6月份的市值\n",
    "    final_merged['ME'] = final_merged['me']\n",
    "    \n",
    "    # --- 步骤 6: 清理并输出 ---\n",
    "    print(\"Step 6: Finalizing output DataFrame...\")\n",
    "    \n",
    "    output_cols = ['gvkey','date','permno','siccd','shrcd','exchcd','fyear','fyr','prc','ret',\n",
    "                   'at','output','capital','investment','investment_alter','delta','debt','ME','dec_me','ci','BM']\n",
    "    annual_factors = final_merged[output_cols].copy()\n",
    "    # annual_factors = final_merged.copy()\n",
    "\n",
    "    annual_factors.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    print(f\"--- Annual Anomaly Creation Complete. Generated {len(annual_factors)} factor observations. ---\")\n",
    "    \n",
    "    return annual_factors, crsp_june"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "897afc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Annual Investment Anomaly Creation ---\n",
      "Step 1: Processing Compustat annual data...\n",
      "Step 2: Preparing and linking via CCM table...\n",
      "Step 3: Creating CRSP June and December anchor subsets...\n",
      "Step 4: Performing end-of-June merge and de-duplication...\n",
      "Step 5: Calculating final annual anomalies...\n",
      "Step 6: Finalizing output DataFrame...\n",
      "--- Annual Anomaly Creation Complete. Generated 250158 factor observations. ---\n"
     ]
    }
   ],
   "source": [
    "prepared_data, crspjune = create_annual_anomalies(compa,crspm,ccm_lnk)\n",
    "\n",
    "# fyear = t-1期末\n",
    "# date = t (June)\n",
    "# t(July) -> t+1 (June): r_{t+1} --  formation_date: t(June)\n",
    "\n",
    "prepared_data['tau_t'] = (prepared_data['fyear']+1).apply(get_statutory_tax_rate)\n",
    "prepared_data['tau_t1'] = (prepared_data['fyear'] + 2).apply(get_statutory_tax_rate)\n",
    "\n",
    "future_lst = ['capital','output','delta','investment','investment_alter','at']\n",
    "prepared_data = compute_future_yearly(prepared_data,future_lst,[],1)\n",
    "prepared_data = compute_future_yearly(prepared_data,future_lst,[],2)\n",
    "\n",
    "prepared_data['y/k_t'] = prepared_data['output_t1'] / prepared_data['capital_t1']\n",
    "# prepared_data['y/k_t1'] = prepared_data['output_t2'] / prepared_data['capital_t1']\n",
    "prepared_data['i/k_t'] = prepared_data['investment_t1'] / prepared_data['capital_t1']\n",
    "prepared_data['i/k_t1'] = prepared_data['investment_t2'] / prepared_data['capital_t2']\n",
    "prepared_data['delta_t'], prepared_data['delta_t1'] = prepared_data['delta_t1'], prepared_data['delta_t2']\n",
    "prepared_data['leverage_t'] = prepared_data['debt'] / (prepared_data['dec_me'] + prepared_data['debt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "239074b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/y5d9zrdx4f191rc45t2grjr80000gn/T/ipykernel_11718/4241849295.py:3: FutureWarning: 'Y-Jun' is deprecated and will be removed in a future version, please use 'Y-JUN' instead.\n",
      "  baa['fyear'] = baa['observation_date'].dt.to_period('Y-Jun').dt.year -1\n"
     ]
    }
   ],
   "source": [
    "# roughly calculate r^B\n",
    "baa = pd.read_csv(FRED_PATH + 'Baayield.csv',parse_dates=['observation_date'])\n",
    "baa['fyear'] = baa['observation_date'].dt.to_period('Y-Jun').dt.year -1\n",
    "baa['r^b'] = baa.groupby('fyear')['BAA'].transform('mean')\n",
    "baa['r^b'] = baa['r^b']/100\n",
    "baa = baa[['fyear','r^b']]\n",
    "baa = baa.drop_duplicates(['fyear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "722da625",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data['fyear_for_bond'] = prepared_data['fyear'] + 1\n",
    "prepared_data = prepared_data.merge(baa, left_on=['fyear_for_bond'], right_on=['fyear'], how='left', suffixes=('','_bond'))\n",
    "prepared_data['r^Ba'] = prepared_data['r^b'] - (prepared_data['r^b']-1)*prepared_data['tau_t1']\n",
    "prepared_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "prepared_data['siccd'] = pd.to_numeric(prepared_data['siccd'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1d7bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['permno','date','fyear','fyr','siccd','shrcd','exchcd','prc','y/k_t','i/k_t','i/k_t1','delta_t','delta_t1','leverage_t','r^Ba','BM','ci']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d366299",
   "metadata": {},
   "outputs": [],
   "source": [
    "BM_group = prepared_data[(prepared_data['fyr'] == 12)\n",
    "                             &(~prepared_data['siccd'].between(4000,4999))\n",
    "                             &(~prepared_data['siccd'].between(6000,6999))\n",
    "                             &(prepared_data['at_t1']> 0)\n",
    "                             &(prepared_data['capital_t1']>0)\n",
    "                             &(prepared_data['output_t1']>0)\n",
    "                             &(prepared_data['leverage_t']>0)\n",
    "                             &(prepared_data['BM']>0)]  \n",
    "BM_group = BM_group[keep_cols]\n",
    "BM_group = BM_group.drop('ci',axis=1)\n",
    "BM_group = BM_group.dropna()\n",
    "CI_group = prepared_data[(prepared_data['fyr'] == 12)\n",
    "                             &(~prepared_data['siccd'].between(4000,4999))\n",
    "                             &(~prepared_data['siccd'].between(6000,6999))\n",
    "                             &(prepared_data['at_t1']> 0)\n",
    "                             &(prepared_data['capital_t1']>0)\n",
    "                             &(prepared_data['output_t1']>0)\n",
    "                             &(prepared_data['leverage_t']>0)]\n",
    "CI_group = CI_group[keep_cols]\n",
    "CI_group = CI_group.drop('BM',axis=1)\n",
    "CI_group = CI_group.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1760dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BM_group.to_csv('BM_group.csv')\n",
    "CI_group.to_csv('CI_group.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ca195e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 数据流失诊断报告 ==========\n",
      "0. 原始 prepared_data 样本量: 177739\n",
      "1. 筛选 fyr=12 后: 105256 (流失率: 40.8%)\n",
      "2. 剔除金融/公用事业后: 71705 (流失率: 31.9%)\n",
      "3. 剔除负资产/负产出后: 64383 (流失率: 10.2%)\n",
      "4. 剔除零杠杆(Zero Debt)后: 56499 (流失率: 12.2%)\n",
      "5. 剔除负 BM 后: 53257 (流失率: 5.7%)\n",
      "--- 准备进入 dropna 环节，当前样本: 53257 ---\n",
      "   变量 y/k_t 缺失数量: 0 (占比: 0.0%)\n",
      "   变量 i/k_t 缺失数量: 663 (占比: 1.2%)\n",
      "   变量 i/k_t1 缺失数量: 4803 (占比: 9.0%)\n",
      "   变量 delta_t1 缺失数量: 4354 (占比: 8.2%)\n",
      "   变量 r^Ba 缺失数量: 0 (占比: 0.0%)\n",
      "6. 最终 dropna 后 (BM_group): 41369\n"
     ]
    }
   ],
   "source": [
    "print(\"========== 数据流失诊断报告 ==========\")\n",
    "prepared_data1 = prepared_data[(prepared_data['date']>=datetime(1963,1,1)) & (prepared_data['date']<=datetime(2005,12,31))]\n",
    "print(f\"0. 原始 prepared_data 样本量: {len(prepared_data1)}\")\n",
    "# 1. 检查 fyr = 12 的影响\n",
    "df_step1 = prepared_data1[prepared_data1['fyr'] == 12]\n",
    "print(f\"1. 筛选 fyr=12 后: {len(df_step1)} (流失率: {1 - len(df_step1)/len(prepared_data1):.1%})\")\n",
    "\n",
    "# 2. 检查行业剔除的影响\n",
    "df_step2 = df_step1[\n",
    "    (~df_step1['siccd'].between(4000,4999)) & \n",
    "    (~df_step1['siccd'].between(6000,6999))\n",
    "]\n",
    "print(f\"2. 剔除金融/公用事业后: {len(df_step2)} (流失率: {1 - len(df_step2)/len(df_step1):.1%})\")\n",
    "\n",
    "# 3. 检查正变量筛选的影响 (at, capital, output > 0)\n",
    "# 通常这些不会剔除太多，除非数据质量很差\n",
    "df_step3 = df_step2[\n",
    "    (df_step2['at_t1'] > 0) & \n",
    "    (df_step2['capital_t1'] > 0) & \n",
    "    (df_step2['output_t1'] > 0)\n",
    "]\n",
    "print(f\"3. 剔除负资产/负产出后: {len(df_step3)} (流失率: {1 - len(df_step3)/len(df_step2):.1%})\")\n",
    "\n",
    "# 4. 检查 Leverage > 0 的影响 (嫌疑人 A)\n",
    "df_step4 = df_step3[df_step3['leverage_t'] > 0]\n",
    "print(f\"4. 剔除零杠杆(Zero Debt)后: {len(df_step4)} (流失率: {1 - len(df_step4)/len(df_step3):.1%})\")\n",
    "\n",
    "# 5. 检查 BM > 0 的影响\n",
    "df_step5 = df_step4[df_step4['BM'] > 0]\n",
    "print(f\"5. 剔除负 BM 后: {len(df_step5)} (流失率: {1 - len(df_step5)/len(df_step4):.1%})\")\n",
    "\n",
    "# 6. 检查 dropna 的影响 (嫌疑人 B - 终极杀手)\n",
    "# 我们先看看还没 dropna 之前有多少\n",
    "print(f\"--- 准备进入 dropna 环节，当前样本: {len(df_step5)} ---\")\n",
    "\n",
    "# 检查关键变量的缺失情况\n",
    "cols_to_check = ['y/k_t', 'i/k_t', 'i/k_t1', 'delta_t1', 'r^Ba']\n",
    "for col in cols_to_check:\n",
    "    num_missing = df_step5[col].isnull().sum()\n",
    "    print(f\"   变量 {col} 缺失数量: {num_missing} (占比: {num_missing/len(df_step5):.1%})\")\n",
    "\n",
    "df_final = df_step5.dropna(subset=keep_cols)\n",
    "print(f\"6. 最终 dropna 后 (BM_group): {len(df_final)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
